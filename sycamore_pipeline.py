# -*- coding: utf-8 -*-
"""sycamore_pipeline_a26c9886-296a-472b-b309-26b366009c71.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Myqj4ygwhMggSA5FoGi41r2xoZYERxLJ
"""

!pip install sycamore-ai[pinecone]
# DocPrep code uses the Sycamore document ETL library: https://github.com/aryn-ai/sycamore

!apt-get install poppler-utils

import pyarrow.fs
import sycamore
import json
import os
from pinecone import Pinecone
from sycamore.functions.tokenizer import OpenAITokenizer
from sycamore.llms import OpenAIModels, OpenAI
from sycamore.transforms import COALESCE_WHITESPACE
from sycamore.transforms.merge_elements import GreedySectionMerger
from sycamore.transforms.partition import ArynPartitioner
from sycamore.transforms.embed import OpenAIEmbedder
from sycamore.materialize_config import MaterializeSourceMode
from sycamore.utils.pdf_utils import show_pages
from sycamore.transforms.summarize_images import SummarizeImages
from sycamore.context import ExecMode
from pinecone import ServerlessSpec

# API Keys
from google.colab import userdata
# Set your secrets in the colab notebook. Navigate to the left pane
# and choose the key option to set your keys. Make sure to enable Notebook access
try:
  # Visit https://www.aryn.ai/get-started to get a key.
  os.environ["ARYN_API_KEY"] = userdata.get('ARYN_API_KEY')
  os.environ["PINECONE_API_KEY"] = userdata.get('PINECONE_API_KEY')
  os.environ["OPENAI_API_KEY"] = userdata.get('OPENAI_API_KEY')
except Exception as e:
  print("YOU ARE MISSING REQUIRED API KEYS FOR THIS PIPELINE. Add your API keys to the Secrets page (icon is a key) in the Colab left navigation panel. It is case sensitive.")

# Sycamore uses lazy execution for efficiency, so the ETL pipeline will only execute when running cells with specific functions.

paths = [""C:\Users\ezaza\Downloads\Digestive_System_508-bbox.json""]

# Initialize the Sycamore context
ctx = sycamore.init(ExecMode.LOCAL)
# Set the embedding model and its parameters
model_name = "text-embedding-3-small"
max_tokens = 8191
dimensions = 1536
# Initialize the tokenizer
tokenizer = OpenAITokenizer(model_name)

ds = (
    ctx.read.binary(paths, binary_format="pdf")
    # Partition and extract tables and images
    .partition(partitioner=ArynPartitioner(
        threshold="auto",
        use_ocr=True,
        extract_table_structure=True,
        extract_images=True,
        source="docprep"
    ))
    # Use materialize to cache output. If changing upstream code or input files, change setting from USE_STORED to RECOMPUTE to create a new cache.
    .materialize(path="/content/materialize/partitioned", source_mode=MaterializeSourceMode.USE_STORED)
    # Merge elements into larger chunks
    .merge(merger=GreedySectionMerger(
      tokenizer=tokenizer,  max_tokens=max_tokens, merge_across_pages=False
    ))
    # Split elements that are too big to embed
    .split_elements(tokenizer=tokenizer, max_tokens=max_tokens)
)

ds.execute()

# Display the first 3 pages after chunking
show_pages(ds, limit=3)

embedded_ds = (
    # Copy document properties to each Document's sub-elements
    ds.spread_properties(["path", "entity"])
    # Convert all Elements to Documents
    .explode()
    # Embed each Document. You can change the embedding model. Make your target vector index matches this number of dimensions.
    .embed(embedder=OpenAIEmbedder(model_name=model_name))
)
# To know more about docset transforms, please visit https://sycamore.readthedocs.io/en/latest/sycamore/transforms.html

# Create an instance of ServerlessSpec with the specified cloud provider and region
spec = ServerlessSpec(cloud="aws", region="us-east-1")
index_name = "quickstart"
# Write data to a Pinecone index
embedded_ds.write.pinecone(index_name=index_name,
    dimensions=dimensions,
    distance_metric="cosine",
    index_spec=spec
)

# Verify data has been loaded using DocSet Query to retrieve chunks
query_docs = ctx.read.pinecone(index_name=index_name, api_key=os.getenv('PINECONE_API_KEY'))
query_docs.show(show_embedding=False)

pip install openai==0.28.0

import os
import openai
import anthropic
from pinecone import Pinecone, ServerlessSpec
from dotenv import load_dotenv

# Load environment variables
load_dotenv()
os.environ["ANTHROPIC_API_KEY"] = os.getenv("Anthropic_API_KEY")
os.environ["PINECONE_API_KEY"] = os.getenv("PINECONE_API_KEY")
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")

# Initialize OpenAI
openai.api_key = os.getenv("OPENAI_API_KEY")

# Initialize Pinecone
pinecone_client = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))

# Specify index name
index_name = "demo"

# Check if the index exists; if not, create it
if index_name not in pinecone_client.list_indexes().names():
    pinecone_client.create_index(
        name=index_name,
        dimension=1536,  # Dimension of OpenAI embeddings
        metric="cosine",
        spec=ServerlessSpec(
            cloud="aws",  # Specify cloud provider
            region="us-west-1"  # Specify region
        )
    )

# Access the index
index = pinecone_client.Index(index_name)

# Initialize Anthropic
anthropic_client = anthropic.Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))

# Function to generate query embedding
def get_embedding(text, model="text-embedding-ada-002"):
    """
    Generate embeddings for the input text using OpenAI's updated API.
    """
    response = openai.Embedding.create(
        model=model,
        input=[text]  # Input must be a list
    )
    return response['data'][0]['embedding']  # Extract the embedding

# Function to query Pinecone and send data to Claude
def ask_claude(user_input):
    """
    Query Pinecone for relevant context and send the prompt to Anthropic Claude.
    """
    try:
        # Generate embedding for the question
        query_embedding = get_embedding(user_input)

        # Query Pinecone for relevant context
        query_result = index.query(
            vector=query_embedding,
            top_k=5,  # Retrieve top 5 relevant matches
            include_metadata=True
        )

        # Extract context from Pinecone results
        context = "\n".join([item['metadata']['text_representation'] for item in query_result['matches']])

        # Combine the context with user input
        prompt = anthropic.HUMAN_PROMPT + f"Context: {context}\n\nUser Question: {user_input}" + anthropic.AI_PROMPT

        # Send the prompt to Claude
        message = anthropic_client.completions.create(
            model="claude-2",
            max_tokens_to_sample=1000,
            temperature=0,
            prompt=prompt
        )

        # Return Claude's response
        return message.completion

    except Exception as e:
        return f"Error: {str(e)}"

# Example Usage
if __name__ == "__main__":
    print("Chatbot Ready!")
    user_question = input("Enter your question: ")
    response = ask_claude(user_question)
    print("\nClaude's Response:")
    print(response)